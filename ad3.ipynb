{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1619244e",
   "metadata": {},
   "source": [
    "**AD3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fb971",
   "metadata": {},
   "source": [
    "Esta es la **actividad dirigida 3**, que consiste en hacer un ejercicio de programación literaria aprovechando el código que hemos usado en programación con **Python** y dónde realizamos **web scrapping**. \n",
    "A continuación el código fuente:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1fff94",
   "metadata": {},
   "source": [
    "## Código Fuente ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c0078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "resultados = []\n",
    "\n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "tags = soup.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    "\n",
    "tags = soup2.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    "\n",
    "tags = soup3.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    "\n",
    "tags = soup4.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    "\n",
    "tags = soup5.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    "\n",
    "tags = soup6.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    "\n",
    "tags = soup7.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    "\n",
    "tags = soup8.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    "\n",
    "tags = soup9.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    "\n",
    "tags = soup10.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    "\n",
    "tags = soup11.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    "\n",
    "tags = soup12.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    "\n",
    "tags = soup13.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    "\n",
    "tags = soup14.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    "\n",
    "tags = soup15.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    "\n",
    "tags = soup16.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    "\n",
    "tags = soup17.findAll(\"h2\")\n",
    "\n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    "\n",
    "\n",
    "os.system(\"clear\")\n",
    "\n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    "\n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20016b8",
   "metadata": {},
   "source": [
    "## Programación literaria ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f040a468",
   "metadata": {},
   "source": [
    "### Módulos internos del sistema ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e7c58",
   "metadata": {},
   "source": [
    "1. [Time](https://docs.python.org/es/3/library/time.html), módulo que proporciona varias funciones relacionadas con el tiempo.<br> \n",
    "2. [csv](https://docs.python.org/3/library/csv.html), formato de importación y exportación más común para hojas de cálculo y bases de datos.<br>\n",
    "3. [re](https://docs.python.org/3/library/re.html), módulo proporciona operaciones de coincidencia de expresiones regulares similares a las que se encuentran en Perl.  \n",
    "4. [os](https://docs.python.org/3/library/os.html), módulo que proporciona una forma portátil de usar la funcionalidad dependiente del sistema operativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb015d0",
   "metadata": {},
   "source": [
    "### Librerías externas ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284e1d8",
   "metadata": {},
   "source": [
    "1. [requests](https://requests.readthedocs.io/en/latest/), biblioteca de python trabajar con HTTP.  \n",
    "2. [bs4](https://pypi.org/project/beautifulsoup4/), librería que facilita extraer información de páginas web.  \n",
    "3. [pandas](https://pypi.org/project/pandas/), paquete de herramientas de análisis de datos de Python.  \n",
    "4. [termcolor](https://pypi.org/project/termcolor/), librería permite imprimir el texto coloreado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8ac82",
   "metadata": {},
   "source": [
    "### Instalamos librerías ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad896a",
   "metadata": {},
   "source": [
    "Las librerías que vienen con Phyton no hay que instalarlas, pero las otras sí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae809341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in f:\\anaconda\\lib\\site-packages (2.27.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bs4 in f:\\anaconda\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: pandas in f:\\anaconda\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: termcolor in f:\\anaconda\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\anaconda\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\anaconda\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in f:\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\anaconda\\lib\\site-packages (from requests) (1.26.9)\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4 in f:\\anaconda\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in f:\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in f:\\anaconda\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in f:\\anaconda\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in f:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\anaconda\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "pip install requests bs4 pandas termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf1b642",
   "metadata": {},
   "source": [
    "### Importamos librerías ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b2ba5",
   "metadata": {},
   "source": [
    "Importamos las necesarias  \n",
    "1. La librería <code>requests</code>  \n",
    "2. La librería <code>bs4</code>  \n",
    "3. La librería <code>pandas</code>\n",
    "4. La librería <code>termcolor</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02aee66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a41a48",
   "metadata": {},
   "source": [
    "## Objetos/variables ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83befe1e",
   "metadata": {},
   "source": [
    "Vamos a crear una serie de objetos o variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2313be",
   "metadata": {},
   "source": [
    "#### Resultados ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c67c7",
   "metadata": {},
   "source": [
    "Creamos un objeto vacío denominado resultados en el que se añadirán los resultados del *scrapping**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74074b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cdc7bd",
   "metadata": {},
   "source": [
    "Si pasamos la función type() podemos observar que este objeto es de tipo \"lista\" de Python. Este tipo de elemento es básico y fundamental para organizar información/datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type (resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b3298",
   "metadata": {},
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2dfe5b",
   "metadata": {},
   "source": [
    "### Solicitud de acceso a datos en un sitio en línea ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c2613",
   "metadata": {},
   "source": [
    "Se agrega una variable para utilizar la solicitud req=requests.get(\"https://resultados.elpais.com\") para recoger la información de esa URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301fe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://resultados.elpais.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a867de79",
   "metadata": {},
   "source": [
    "Si pasamos la función type() podemos observar que este objeto es de tipo Respuesta de modelo requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18253940",
   "metadata": {},
   "source": [
    "Si pasamos la función type() podemos observar que este objeto es de tipo Respuesta de modelo requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "Se utiliza el condicional if para que cuando el estatus no sea 200 el valor de vuelta no pueda leerse. El valor 200 significa que la solicitud fue exitosa y el servidor respondió con los datos solicitados. De no encontrarse un resultado como el indicado se mostrará el mensaje No se puede hacer Web Scrapping en seguido de la URL errónea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ad8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si el estatus no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    "    raise Exception(\"No se puede hacer Web Scraping en\"+ URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b341d7",
   "metadata": {},
   "source": [
    "### Beautiful Soup ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeba96f",
   "metadata": {},
   "source": [
    "Agregamos otra variable para aplicar el Web Scrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed0e254",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe51bd",
   "metadata": {},
   "source": [
    "Ademas agregamos la variable que funcioana para encontrar todas las etiquetas \"h2\" dentro de la página anteriormente especificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ce903",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.findAll(\"h2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedc742",
   "metadata": {},
   "outputs": [],
   "source": [
    "Se indica a través de un for, que se encuentren todos los h2 dentro de la variable anterior y se imprima el texto dentro de la etiqueta. Luego con resultados.append(h2.text) se añaden todos los textos dentro de las etiquetas a la lista inicial resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
